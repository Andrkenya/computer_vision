{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Bukenya Andrew\n",
        "#2021/HD05/2288U\n",
        "#2100702288\n",
        "\n",
        "#Classifying with Support Vector Machines(SVM)"
      ],
      "metadata": {
        "id": "kUDbAvL4mc07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mounting Google drive"
      ],
      "metadata": {
        "id": "0iQ2vPUgIo5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmjTVo7wnSjS",
        "outputId": "161bb599-e0f4-4b1f-acfe-675ea5e9475f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/AndrewsProject1/project_files/crop_classisfication"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIJDG9Vzo6Qr",
        "outputId": "22334fbb-191d-48ad-9b74-ed8ff42793bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/AndrewsProject1/project_files/crop_classisfication\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required modules\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import svm\n",
        "import joblib\n",
        "\n",
        "def data(X_path,Y_path):\n",
        "  \"\"\"Restore X and Y lists from the .picle files save initially\"\"\"\n",
        "  # Open the X.pickle file\n",
        "  pickle_in = open(X_path, \"rb\")\n",
        "  X = pickle.load(pickle_in)\n",
        "  # Open the Y.pickle file\n",
        "  pickle_in = open(Y_path, \"rb\")\n",
        "  Y = pickle.load(pickle_in)\n",
        "  return X, Y\n",
        "\n",
        "def sift(img):\n",
        "  \"\"\"Creation of  SIFT method that extracts features, and it return kp and des\"\"\"\n",
        "  # Creating SIFT method\n",
        "  sift = cv2.xfeatures2d.SIFT_create()\n",
        "  # Determining the number of features and features\n",
        "  kp, des = sift.detectAndCompute(img,None)\n",
        "  return kp, des\n",
        "\n",
        "def surf(img):\n",
        "  \"\"\"Create SURF method to extract features, and it returns kp and des\"\"\"\n",
        "  # Create SURF method\n",
        "  surf = cv2.xfeatures2d.SURF_create()\n",
        "  # Determining the number of features and features\n",
        "  kp, des = surf.detectAndCompute(img,None)\n",
        "  return kp, des\n",
        "\n",
        "#Adding ORB algorithm to extract more features\n",
        "def orb(img):\n",
        "  \"\"\"Creation of  ORB method and it return kp and des\"\"\"\n",
        "  # Create ORB method\n",
        "  orb = cv2.ORB_create()\n",
        "  # Determining the number of features and features\n",
        "  kp, des = orb.detectAndCompute(img,None)\n",
        "  return kp, des\n",
        "\n",
        "\n",
        "def feature_number(feature):\n",
        "  \"\"\"Creation of a list with image features, it returns list_data and ind\"\"\"\n",
        "  # Creating a blank list ind\n",
        "  ind = []\n",
        "  # Create a blank list_data list\n",
        "  list_data = []\n",
        "  t0 = time()\n",
        "  # Iteration from 0 to the total number of data in X\n",
        "  for i in range(len(X)):\n",
        "    # Execution of SIFT, SURF and ORB functions\n",
        "    kp, des = feature(X[i])\n",
        "    # If the number of features in that image is less than 20, the image does not qualify\n",
        "    if len(kp) < 20:\n",
        "      # Adding to ind list\n",
        "      ind.append(i)\n",
        "      continue\n",
        "    # Forming a feature of equal size (equal number of data)\n",
        "    des = des[0:20,:]\n",
        "    # Formation of the obtained feature data in the form 1, len (des) * len (des [1])\n",
        "    vector_data = des.reshape(1,len(des)*len(des[1]))\n",
        "    # Adding vector_data to the list_data list\n",
        "    list_data.append(vector_data)\n",
        "  # List of names of feature extraction methods\n",
        "  features = ['sift', 'surf', 'orb']\n",
        "  print(\"Algorithm time: %0.3fs\" % (time() - t0))\n",
        "  return list_data, ind\n",
        "    \n",
        "def svm_parameters(X_train, y_train):\n",
        "  \"\"\"Finding model training parameters and returning clf.best_estimator\"\"\"\n",
        "  t0 = time()\n",
        "  # Parameters\n",
        "  param_grid = {'C': [1e2, 1e3, 5e3, 1e4, 5e4, 1e5],\n",
        "              'gamma': [0.0001, 0.001, 0.01, 0.1], \n",
        "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "  # Parameter search function\n",
        "  clf = GridSearchCV(\n",
        "    svm.SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "  print(\"Parameter finding time: %0.3fs\" % (time() - t0))\n",
        "  return clf.best_estimator_\n",
        "\n",
        "def svm_train(X_train, y_train):\n",
        "  \"\"\"Model training and returning clf\"\"\"\n",
        "  t0 = time()\n",
        "  # Creating an SVM classifier\n",
        "  clf = svm.SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
        "    decision_function_shape='ovr', degree=3, gamma=1e-8, kernel='rbf',\n",
        "    max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
        "  # Training of SVM classification model\n",
        "  clf.fit(X_train, y_train)\n",
        "  print(\"Model training time: %0.3fs\" % (time() - t0))\n",
        "  return clf\n",
        "\n",
        "def svm_test(clf, X_test, y_test):\n",
        "  \"\"\"Testing the model and returning y_pred\"\"\"\n",
        "  t0 = time()\n",
        "  # Testing of SVM classification model\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  # Model accuracy: what percentage is accurately classified data? (TP + TN) / (TP + TN + FP + FN)\n",
        "  print(\"\\n Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "  # Model precision: what is the percentage of positive identifications in a set of positively classified data? TP / (TP + FP)\n",
        "  print(\"\\n Precision:\",metrics.precision_score(y_test, y_pred, average='micro'))\n",
        "\n",
        "  # Model recall: what is the percentage of positive identifications in the set of all positive data? TP / (TP + FN)\n",
        "  print(\"\\n Recall:\",metrics.recall_score(y_test, y_pred, average='micro'))\n",
        "\n",
        "  # Results table or classification report\n",
        "  print(\"\\n CLASSIFICATION REPORT\")\n",
        "\n",
        "  print(classification_report(y_test, y_pred, target_names=categories))\n",
        "  print(\"\\nModel testing time: %0.3fs\" % (time() - t0))\n",
        "  return y_pred\n",
        "\n",
        "def svm_save(clf, path):\n",
        "  \"\"\"Saving SVM model\"\"\"\n",
        "  joblib.dump(clf, path)\n",
        "\n",
        "if __name__ == '__main__': \n",
        "  # List of categories\n",
        "  categories = [\"banana\", \"cassava\"]\n",
        "  # The directory where the X and Y data is located\n",
        "  X_path = \"/content/gdrive/MyDrive/AndrewsProject1/project_files/X.pickle\"\n",
        "  Y_path = \"/content/gdrive/MyDrive/AndrewsProject1/project_files/Y.pickle\"\n",
        "\n",
        "  # Image width\n",
        "  IMG_W = int(161)\n",
        "  # Image height\n",
        "  IMG_H = int(150)\n",
        "\n",
        "  # Executing the data() function\n",
        "  X, Y = data(X_path, Y_path)\n",
        "  # List of names of feature extraction methods\n",
        "  features = ['banana', 'cassava']\n",
        "  a = 0\n",
        "  # Iterate through individual features\n",
        "  for feature in [sift, surf, orb]:\n",
        "    t1 = time()\n",
        "\n",
        "    # Copying data from Y\n",
        "    labels = Y[:]\n",
        "    # Executing the feature_number() function\n",
        "    list_data, ind = feature_number(feature)\n",
        "    # Iterate through the list to delete data that didn't meet a sufficient number of features.\n",
        "    for i in sorted(ind, reverse=True):\n",
        "      del labels[i]\n",
        "\n",
        "    # Creating a vector in the form of len (labels), len (list_data [0] [0])\n",
        "    data = np.array(list_data).reshape(len(labels),len(list_data[0][0]))\n",
        "\n",
        "    # Creating a vector\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(labels)\n",
        "\n",
        "    # Division of dataset into trained and tested data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3,random_state=42) # 70% training and 30% test\n",
        "\n",
        "    # Executing the svm_train() function\n",
        "    clf = svm_train(X_train, y_train)\n",
        "\n",
        "    # Executing the svm_test() function\n",
        "    y_pred = svm_test(clf, X_test, y_test)\n",
        "    print(\"The program executed in: %0.3fs\" % (time() - t1))\n",
        "\n",
        "    # Saving the model to a directory\n",
        "    save_path = \"/content/gdrive/MyDrive/AndrewsProject1/project_files\" + str(features[a]) + \"svm_trained_model.npy\"\n",
        "    # Executing the svm_save() function\n",
        "    svm_save(clf, save_path)\n",
        "    a += 1\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "5HvzAhscCx6B",
        "outputId": "1053ec6a-2db9-401e-cc16-8255e35cb827"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Algorithm time: 2.157s\n",
            "Model training time: 0.011s\n",
            "\n",
            " Accuracy: 0.9166666666666666\n",
            "\n",
            " Precision: 0.9166666666666666\n",
            "\n",
            " Recall: 0.9166666666666666\n",
            "\n",
            " CLASSIFICATION REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      banana       1.00      0.82      0.90        17\n",
            "     cassava       0.86      1.00      0.93        19\n",
            "\n",
            "    accuracy                           0.92        36\n",
            "   macro avg       0.93      0.91      0.92        36\n",
            "weighted avg       0.93      0.92      0.92        36\n",
            "\n",
            "\n",
            "Model testing time: 0.019s\n",
            "The program executed in: 2.194s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1f5f0e7a274d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# Executing the feature_number() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mlist_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Iterate through the list to delete data that didn't meet a sufficient number of features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1f5f0e7a274d>\u001b[0m in \u001b[0;36mfeature_number\u001b[0;34m(feature)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Execution of SIFT, SURF and ORB functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mkp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;31m# If the number of features in that image is less than 20, the image does not qualify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1f5f0e7a274d>\u001b[0m in \u001b[0;36msurf\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;34m\"\"\"Create SURF method to extract features, and it returns kp and des\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m# Create SURF method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0msurf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSURF_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0;31m# Determining the number of features and features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mkp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv_contrib/modules/xfeatures2d/src/surf.cpp:1027: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'create'\n"
          ]
        }
      ]
    }
  ]
}